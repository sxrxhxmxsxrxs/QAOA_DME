{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1. Data Structures and Dependency Management\n",
    "\n",
    "dataclasses is being used for structured data storage and scipy.spatial for efficient distance matrix computation. Each instance encapsulates the spatial coordinates, the corresponding adjacency matrix, and the exact global extrema (optimal and worst-case tour lengths)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cb90ee606587d0"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import scipy\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "@dataclass\n",
    "class TSPInstance:\n",
    "    \"\"\"\n",
    "    Data container for a TSP instance and its theoretical bounds.\n",
    "    \n",
    "    Attributes:\n",
    "        kind: The distribution topology ('circle' or 'random').\n",
    "        N: Number of nodes (cities).\n",
    "        sigma: Standard deviation of noise applied to circular coordinates.\n",
    "        coords: (N, 2) array of Cartesian coordinates.\n",
    "        D: (N, N) Euclidean distance matrix.\n",
    "        l_star: Optimal (minimal) Hamiltonian cycle length.\n",
    "        l_worst: Maximal Hamiltonian cycle length.\n",
    "    \"\"\"\n",
    "    kind: str\n",
    "    N: int\n",
    "    sigma: Optional[float]\n",
    "    coords: np.ndarray\n",
    "    D: np.ndarray\n",
    "    l_star: float\n",
    "    l_worst: float"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-13T12:34:27.047170604Z",
     "start_time": "2026-02-13T12:34:26.850571481Z"
    }
   },
   "id": "6f2c16414a5c5495",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Environment Documentation and Reproducibility\n",
    "\n",
    "In computational research, documenting the software environment is critical for the reproducibility of results. Discrepancies in library versions (e.g., NumPy or SciPy) can lead to subtle variations in floating-point arithmetic or algorithmic behavior.\n",
    "\n",
    "This section automates the provenance tracking by:\n",
    "\n",
    "    Generating a requirements.txt file: Capturing the exact state of the pip environment.\n",
    "\n",
    "    Logging System Metadata: Recording the Python interpreter version, operating system architecture, and core dependency versions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f45cf80eb7773e3"
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "def get_environment_info():\n",
    "    \"\"\"\n",
    "    Captures the runtime environment metadata for scientific documentation.\n",
    "    Ensures that the exact versions of computational libraries are known.\n",
    "    \"\"\"\n",
    "    info = {\n",
    "        \"python_version\": sys.version,\n",
    "        \"platform\": platform.platform(),\n",
    "        \"processor\": platform.processor(),\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"scipy_version\": scipy.__version__\n",
    "    }\n",
    "    return info\n",
    "\n",
    "# Automated generation of requirements.txt for environment replication\n",
    "with open(\"requirements_generate_dataset.txt\", \"w\") as f:\n",
    "    # Captures all installed packages via pip freeze\n",
    "    subprocess.run([\"pip\", \"freeze\"], stdout=f)\n",
    "\n",
    "# Display and store environment metadata\n",
    "env_info = get_environment_info()\n",
    "print(\"--- Runtime Environment Captured ---\")\n",
    "for key, value in env_info.items():\n",
    "    print(f\"{key}: {value}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-13T12:34:27.323285639Z",
     "start_time": "2026-02-13T12:34:27.049967427Z"
    }
   },
   "id": "df6582501c423af4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Runtime Environment Captured ---\n",
      "python_version: 3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:09:02) [GCC 11.2.0]\n",
      "platform: Linux-6.1.0-38-amd64-x86_64-with-glibc2.36\n",
      "processor: \n",
      "numpy_version: 2.4.2\n",
      "scipy_version: 1.17.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Compute tour lengths\n",
    "To establish the ground truth, the Held-Karp algorithm is being used to determine the exact optimal tour length (l_star) and the worst-case tour length (l_worst)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab556d637da8cebb"
  },
  {
   "cell_type": "code",
   "source": [
    "def solve_tsp_extremes(D: np.ndarray) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculates the minimum and maximum Hamiltonian cycles using Dynamic Programming.\n",
    "    \"\"\"\n",
    "    n = len(D)\n",
    "    memo_min = {}\n",
    "    memo_max = {}\n",
    "\n",
    "    def visit(mask, last, find_max=False):\n",
    "        memo = memo_max if find_max else memo_min\n",
    "        \n",
    "        # Termination: All nodes visited; return to origin\n",
    "        if mask == (1 << n) - 1:\n",
    "            return D[last][0]\n",
    "        \n",
    "        if (mask, last) in memo:\n",
    "            return memo[(mask, last)]\n",
    "\n",
    "        if find_max:\n",
    "            res = max(D[last][i] + visit(mask | (1 << i), i, True)\n",
    "                      for i in range(n) if not (mask & (1 << i)))\n",
    "        else:\n",
    "            res = min(D[last][i] + visit(mask | (1 << i), i, False)\n",
    "                      for i in range(n) if not (mask & (1 << i)))\n",
    "\n",
    "        memo[(mask, last)] = res\n",
    "        return res\n",
    "\n",
    "    return visit(1, 0, False), visit(1, 0, True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-13T12:34:27.333477790Z",
     "start_time": "2026-02-13T12:34:27.324780815Z"
    }
   },
   "id": "93583ce2d5ab56ae",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Coordinate Generation and Topologies\n",
    "- Perturbed Circular Distribution: Points are distributed along a ring of radius R=N/2π and then Gaussian noise, controlled by σ to vary the difficulty of the manifold, is introduced.\n",
    "- Uniform Random Distribution: Points are sampled from a uniform distribution U(0,1) in a 2D unit square."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da792b0ea6fda36c"
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_cities(N: int, sigma: float, rng: np.random.Generator) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generates a circular distribution of cities with additive noise.\"\"\"\n",
    "    R = N / (2 * np.pi)\n",
    "    angles = np.linspace(0, 2 * np.pi, N, endpoint=False)\n",
    "\n",
    "    rand_phi = rng.uniform(0, 2 * np.pi, N)\n",
    "    rand_r = rng.uniform(0, sigma, N)\n",
    "\n",
    "    coords = np.zeros((N, 2))\n",
    "    coords[:, 0] = R * np.cos(angles) + rand_r * np.cos(rand_phi)\n",
    "    coords[:, 1] = R * np.sin(angles) + rand_r * np.sin(rand_phi)\n",
    "\n",
    "    D = distance_matrix(coords, coords)\n",
    "    return coords, D\n",
    "\n",
    "def generate_random_cities(N: int, rng: np.random.Generator) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generates cities within a uniform unit-square distribution.\"\"\"\n",
    "    coords = rng.random((N, 2))\n",
    "    D = distance_matrix(coords, coords)\n",
    "    return coords, D"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-13T12:34:27.345500218Z",
     "start_time": "2026-02-13T12:34:27.334881316Z"
    }
   },
   "id": "84345fac838df8fa",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Complexity Filtering via Greedy Heuristics\n",
    "\n",
    "To ensure the dataset consists of non-trivial instances, a Greedy Filter is applied. An instance is discarded if a simple Nearest Neighbor (Greedy) heuristic, starting from any node, can achieve the optimal solution l_star.\n",
    "\n",
    "Selection Criterion: Only \"hard\" instances where Greedy(D,s)>l∗ for all starting nodes s ∈ {1,…,N} are retained."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca26fc68e11db02a"
  },
  {
   "cell_type": "code",
   "source": [
    "def fast_greedy(D: np.ndarray, start: int) -> float:\n",
    "    \"\"\"Computes the TSP tour length using the Nearest Neighbor heuristic.\"\"\"\n",
    "    n = D.shape[0]\n",
    "    visited = np.zeros(n, dtype=bool)\n",
    "    visited[start] = True\n",
    "    cur = start\n",
    "    tour_len = 0.0\n",
    "\n",
    "    for _ in range(n - 1):\n",
    "        dist_to_others = D[cur].copy()\n",
    "        dist_to_others[visited] = np.inf\n",
    "        nxt = np.argmin(dist_to_others)\n",
    "        tour_len += D[cur, nxt]\n",
    "        visited[nxt] = True\n",
    "        cur = nxt\n",
    "\n",
    "    return tour_len + D[cur, start]\n",
    "\n",
    "def greedy_filter(D: np.ndarray, l_star: float, atol: float = 1e-7) -> bool:\n",
    "    \"\"\"Returns True if the instance is computationally non-trivial for greedy solvers.\"\"\"\n",
    "    n = D.shape[0]\n",
    "    for s in range(n):\n",
    "        if abs(fast_greedy(D, s) - l_star) < atol:\n",
    "            return False\n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-13T12:34:27.355326407Z",
     "start_time": "2026-02-13T12:34:27.346871104Z"
    }
   },
   "id": "b983da327fe24f91",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Automated Dataset Synthesis\n",
    "\n",
    "The following routine automates the generation, filtering, and validation process across multiple noise levels (σ)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b815e936185c135"
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_dataset_per_N(\n",
    "        N: int,\n",
    "        sigmas: Tuple[float, float, float] = (0.6, 1.0, 1.4),\n",
    "        n_circle_total: int = 150,\n",
    "        n_random: int = 50,\n",
    "        seed: int = 42,\n",
    "        max_trials_per_bucket: int = 100000\n",
    ") -> List[TSPInstance]:\n",
    "    \"\"\"Synthesizes a balanced dataset of filtered TSP instances.\"\"\"\n",
    "    per_sigma = n_circle_total // len(sigmas)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    dataset: List[TSPInstance] = []\n",
    "\n",
    "    # Process Circle-based instances\n",
    "    for sigma in sigmas:\n",
    "        kept = 0\n",
    "        trials = 0\n",
    "        while kept < per_sigma:\n",
    "            trials += 1\n",
    "            if trials > max_trials_per_bucket:\n",
    "                raise RuntimeError(f\"Convergence failure: max trials exceeded for sigma={sigma}\")\n",
    "\n",
    "            coords, D = generate_cities(N, sigma=sigma, rng=rng)\n",
    "            l_star, l_worst = solve_tsp_extremes(D)\n",
    "\n",
    "            if greedy_filter(D, l_star):\n",
    "                inst = TSPInstance(\"circle\", N, sigma, coords, D, l_star, l_worst)\n",
    "                dataset.append(inst)\n",
    "                kept += 1\n",
    "\n",
    "    # Process Uniform Random instances\n",
    "    kept = 0\n",
    "    while kept < n_random:\n",
    "        coords, D = generate_random_cities(N, rng)\n",
    "        l_star, l_worst = solve_tsp_extremes(D)\n",
    "\n",
    "        if greedy_filter(D, l_star):\n",
    "            inst = TSPInstance(\"random\", N, None, coords, D, l_star, l_worst)\n",
    "            dataset.append(inst)\n",
    "            kept += 1\n",
    "\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-13T12:34:27.366055061Z",
     "start_time": "2026-02-13T12:34:27.356907454Z"
    }
   },
   "id": "9c96b931bc5e692d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. Serialization and Output\n",
    "\n",
    "The final dataset is serialized into a compressed NumPy format (.npz) for efficient storage and downstream analysis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24c06996ccab13f7"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "def save_dataset_npz(filename: str, dataset: List[TSPInstance], env_info: dict) -> None:\n",
    "    \"\"\"\n",
    "    Ensures the output directory exists and serializes the TSP instances\n",
    "    to a compressed archive.\n",
    "    \"\"\"\n",
    "    target_dir = \"Generated_datasets\"\n",
    "\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "        print(f\"Created directory: {target_dir}\")\n",
    "\n",
    "    full_path = os.path.join(target_dir, filename)\n",
    "\n",
    "    # Serialize environmental metadata for reproducibility\n",
    "    env_string = str(env_info)\n",
    "\n",
    "    np.savez_compressed(\n",
    "        full_path,\n",
    "        kinds=np.array([inst.kind for inst in dataset]),\n",
    "        Ns=np.array([inst.N for inst in dataset]),\n",
    "        sigmas=np.array([inst.sigma if inst.sigma is not None else -1.0 for inst in dataset]),\n",
    "        coords=np.array([inst.coords for inst in dataset]),\n",
    "        distances=np.array([inst.D for inst in dataset]),\n",
    "        l_stars=np.array([inst.l_star for inst in dataset]),\n",
    "        l_worsts=np.array([inst.l_worst for inst in dataset]),\n",
    "        metadata=np.array([env_string])\n",
    "    )\n",
    "    print(f\"Dataset successfully saved to: {full_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-13T12:36:03.611801631Z",
     "start_time": "2026-02-13T12:36:03.601754770Z"
    }
   },
   "id": "45190e2815ecf4cb",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    num_cities = 8\n",
    "    seed = 42\n",
    "    #start_t = time.perf_counter()\n",
    "\n",
    "    print(f\"Executing generation for N={num_cities}...\")\n",
    "    dataset = generate_dataset_per_N(N=num_cities, seed=seed)\n",
    "\n",
    "    filename = f\"tsp_benchmark_{num_cities}_cities_seed_{seed}.npz\"\n",
    "    save_dataset_npz(filename, dataset, env_info)\n",
    "\n",
    "    #execution_time = time.perf_counter() - start_t\n",
    "    #print(f\"Total Execution Time: {execution_time:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-13T13:18:12.850763067Z"
    }
   },
   "id": "b363e0bf510612d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing generation for N=8...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note on Loading and Inspecting Compressed Datasets\n",
    "\n",
    "Once the TSP instances and environment metadata are serialized into a .npz archive, they can be retrieved for analysis or further simulations. The np.load function provides a lazy-loading interface, allowing access to specific arrays by their keys (e.g., distances, l_stars) without loading the entire file into memory at once.\n",
    "\n",
    "To ensure compatibility with archived metadata, use allow_pickle=True"
   ],
   "id": "69a361c722b23d01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path = os.path.join(\"Generated_datasets\", \"tsp_benchmark_9_cities_seed_42.npz\")\n",
    "\n",
    "with np.load(file_path, allow_pickle=True) as data:\n",
    "    # 1. List all available keys in the archive\n",
    "    print(\"Available keys:\", data.files)\n",
    "\n",
    "    # 2. Access  TSP instances data\n",
    "    distances = data['distances']\n",
    "    l_stars = data['l_stars']\n",
    "    coords = data['coords']\n",
    "\n",
    "    # 3. Retrieve and print environmental metadata\n",
    "    # (Since it was saved as a single-element array, access index 0)\n",
    "    metadata = data['metadata'][0]\n",
    "\n",
    "    print(f\"\\nSuccessfully loaded {len(distances)} instances.\")\n",
    "    print(f\"Optimal lengths (l_stars) for first 3 instances: {l_stars[:3]}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Environment Metadata:\\n{metadata}\")"
   ],
   "id": "f256be58597beb93"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
